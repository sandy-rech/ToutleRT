{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import re\n",
    "from typing import Union, List, Tuple\n",
    "\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pytz\n",
    "from dateutil import tz\n",
    "\n",
    "\n",
    "sns.set(rc={'axes.facecolor':'whitesmoke'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet(file_path: str,\n",
    "                   output_directory: str):\n",
    "    \"\"\"Convert CSV files to Parquet file format.\"\"\"\n",
    "\n",
    "    # create output file name\n",
    "    output_parquet_file = os.path.join(output_directory, f\"{os.path.splitext(os.path.basename(file_path))[0]}.parquet\")\n",
    "    \n",
    "    # construct query to convert to parquet files\n",
    "    sql = f\"\"\"\n",
    "    COPY(\n",
    "        SELECT\n",
    "            *\n",
    "        FROM \n",
    "            '{file_path}'\n",
    "        ) TO '{output_parquet_file}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "    \n",
    "    # execute conversion\n",
    "    duckdb.query(sql)\n",
    "    \n",
    "    return output_parquet_file\n",
    "\n",
    "\n",
    "def read_tagging_file(tagging_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read in raw excel tagging file to data frame and add field for PST date time fields.\"\"\"\n",
    "    \n",
    "    # read in tagging data\n",
    "    df = pd.read_excel(tagging_file)\n",
    "\n",
    "    # rename fields\n",
    "    #df.rename(columns={\"rel_datetime\": \"tag_release_date\"}, inplace=True)\n",
    "    df.rename(columns={\"rel_date_time\": \"tag_release_date\"}, inplace=True)\n",
    "\n",
    "\n",
    "    # adjust date times in tagging file to PST\n",
    "    df[\"tag_activation_date_pst\"] = pd.to_datetime(df[\"tag_activation_date\"]) - pd.Timedelta(hours=1)\n",
    "    df[\"tag_release_date_pst\"] = pd.to_datetime(df[\"tag_release_date\"]) - pd.Timedelta(hours=1)\n",
    "    \n",
    "    return df.sort_values(by=\"fish_id\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def read_beacon_file(beacon_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read in raw beacon file to data frame.\"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_directory_list(target_directory: str,\n",
    "                            ignore_dirs: Tuple[str] = (\".DS_Store\", \"pre_study\", \"zips\", \".zip\")) -> list:\n",
    "    \"\"\"Generate a clearn directory list.\"\"\"\n",
    "    \n",
    "    return [os.path.join(target_directory, i) for i in os.listdir(target_directory) if i not in ignore_dirs]\n",
    "\n",
    "\n",
    "def add_trailing_zeros(x: str, \n",
    "                       length: int):\n",
    "    \"\"\"Add trailing zeros to a string.\"\"\"\n",
    "    \n",
    "    return(x + f\"{'0' * (length - len(x))}\")\n",
    "\n",
    "\n",
    "def add_leading_zeros(x: str, \n",
    "                    length: int):\n",
    "    \"\"\"Add leading zeros to a string.\"\"\"\n",
    "    \n",
    "    return(f\"{'0' * (length - len(x))}\" + x)\n",
    "\n",
    "\n",
    "def validate_file_to_directory_match(file_list: list):\n",
    "    \"\"\"Validate file names to their parent directory to ensure that no errors have occurred.\"\"\"\n",
    "\n",
    "    valid_list = []\n",
    "    \n",
    "    for i in file_list:\n",
    "\n",
    "        # get the source directory name of the download\n",
    "        source_directory = os.path.basename(os.path.dirname(i))\n",
    "\n",
    "        # extact the file parts to match to source directory\n",
    "        target_file_parts = os.path.splitext(os.path.basename(i))\n",
    "        target_file_base = target_file_parts[0].split(\"_\")[-1]\n",
    "        target_file_extension = target_file_parts[1]\n",
    "\n",
    "        if source_directory != target_file_base:\n",
    "            print(f\"File '{i}' does not match the parent directory name.\")\n",
    "            print(\"Removing from inputs.  Please review.\")\n",
    "\n",
    "        else:\n",
    "            valid_list.append(i)\n",
    "\n",
    "    return valid_list\n",
    "\n",
    "\n",
    "def generate_orion_import_file_list(orion_dir: str):\n",
    "    \"\"\"Generate a list of orion files to import after validation.\"\"\"\n",
    "\n",
    "    # generate full path lists of text and hex files in the orion directory\n",
    "    text_files = glob.glob(os.path.join(orion_dir, \"**/*.txt\"))\n",
    "    hex_files = glob.glob(os.path.join(orion_dir, \"**/*.hex\"))\n",
    "\n",
    "    # validate file name to directory name match\n",
    "    text_files = validate_file_to_directory_match(text_files)\n",
    "    hex_files = validate_file_to_directory_match(hex_files)\n",
    "\n",
    "    # validate to ensure a hex / text pair\n",
    "    text_file_base_list = [os.path.splitext(os.path.basename(i))[0] for i in text_files]\n",
    "    hex_file_base_list = [os.path.splitext(os.path.basename(i))[0] for i in hex_files]\n",
    "\n",
    "    # files in text list not in hex list\n",
    "    no_match_text_files = set(text_file_base_list) - set(hex_file_base_list)\n",
    "\n",
    "    # files in hex list not in text list\n",
    "    no_match_hex_files = set(hex_file_base_list) - set(text_file_base_list)\n",
    "\n",
    "    if len(no_match_text_files) > 0:\n",
    "        print(f\"There are not hex file matches for the following text files: {no_match_text_files}\")\n",
    "\n",
    "    if len(no_match_hex_files) > 0:\n",
    "        print(f\"There are not text file matches for the following hex files: {no_match_hex_files}\")\n",
    "\n",
    "    # hex file size should be smaller than the text file or something may be wrong\n",
    "    text_file_sizes = [os.stat(i).st_size for i in text_files]\n",
    "    hex_file_sizes = [os.stat(i).st_size for i in hex_files]\n",
    "\n",
    "    for index, i in enumerate(text_files):\n",
    "        \n",
    "        text_file_size = os.stat(i).st_size\n",
    "        hex_file_size = os.stat(f\"{os.path.splitext(i)[0]}.hex\").st_size\n",
    "\n",
    "        # if hex_file_size >= text_file_size:\n",
    "        #     print(f\"WARNING:  Text file '{i}' is smaller than or equal to the hex file size (bytes).\")\n",
    "        #     print(f\"Text file size: {text_file_size}, Hex file size: {hex_file_size}, Difference (hex - text): {hex_file_size - text_file_size}\")\n",
    "        #     print(f\"Removing files from import.  Please review.\")\n",
    "\n",
    "        #     # remove files with incorrect sizes\n",
    "        #     text_files.remove(text_files[index])\n",
    "            \n",
    "    return text_files\n",
    "\n",
    "\n",
    "def whitespace_to_csv(input_file:str, \n",
    "                      output_dir:str) -> str:\n",
    "    \"\"\"Generate new output files with whitespace converted to CSV.\"\"\"\n",
    "    \n",
    "    # extract basename from input file\n",
    "    basename = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    \n",
    "    # construct output file name\n",
    "    output_file = os.path.join(output_dir, f\"{basename}.csv\")\n",
    "    \n",
    "    # open file to write\n",
    "    with open(output_file, \"w\") as out:\n",
    "        \n",
    "        # read input file as string\n",
    "        with open(input_file) as get:\n",
    "            content = get.read()\n",
    "            \n",
    "            # write content replacing any whitespace with commas but keeping new lines or carriage returns\n",
    "            out.write(re.sub(\"[^\\S^\\r\\n]+\", \",\", content))\n",
    "            \n",
    "    return output_file\n",
    "\n",
    "\n",
    "def orion_raw_to_parquet(input_file: str,\n",
    "                         output_directory: str,\n",
    "                         target_frequency_list: list,\n",
    "                         target_code_list: list,\n",
    "                         site_to_receiver_dict: dict,\n",
    "                         target_year=int) -> str:\n",
    "    \"\"\"Create a parquet file for each formatted CSV.\"\"\"\n",
    "       \n",
    "    basename = os.path.basename(input_file)\n",
    "    \n",
    "    # extract expected site number from file name\n",
    "    expected_site_number = int(basename.split(\"_\")[0][1:])\n",
    "    \n",
    "    # get expected receiver id\n",
    "    expected_receiver = site_to_receiver_dict[expected_site_number]\n",
    "    \n",
    "    # extract file name from input file\n",
    "    file_name = f\"0_ORION_{os.path.splitext(basename)[0]}\"\n",
    "    \n",
    "    # create output file name\n",
    "    output_parquet_file = os.path.join(output_directory, f\"{file_name}.parquet\")\n",
    "\n",
    "    # get DST start and end\n",
    "    dst_start, dst_end = get_dst_dates(target_year)\n",
    "    dst_start_str = dst_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    dst_end_str = dst_end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    COPY(\n",
    "        SELECT\n",
    "            concat(\n",
    "                CASE\n",
    "                    WHEN length(Freq::VARCHAR) = 4\n",
    "                    THEN Freq::VARCHAR || '000'\n",
    "                    WHEN length(Freq::VARCHAR) = 5\n",
    "                    THEN Freq::VARCHAR || '00'        \n",
    "                    WHEN length(Freq::VARCHAR) = 6\n",
    "                    THEN Freq::VARCHAR || '0' \n",
    "                    ELSE Freq::VARCHAR \n",
    "                END\n",
    "                ,'.'\n",
    "                ,CASE\n",
    "                    WHEN length(Code::VARCHAR) = 1\n",
    "                    THEN '00' || Code::VARCHAR\n",
    "                    WHEN length(Code::VARCHAR) = 2\n",
    "                    THEN '0' || Code::VARCHAR  \n",
    "                    ELSE Code::VARCHAR\n",
    "                END\n",
    "            ) AS fish_id,\n",
    "            CASE\n",
    "                WHEN (Date + Time) >= '{dst_start_str}' AND (Date + Time) <= '{dst_end_str}'\n",
    "                THEN (Date + Time) - INTERVAL '1 HOUR'\n",
    "                ELSE (Date + Time)\n",
    "            END AS date_time,\n",
    "            Site AS receiver_id,\n",
    "            Power AS signal_power,\n",
    "            '{file_name}' AS file_nm\n",
    "        FROM\n",
    "            read_csv('{input_file}')\n",
    "        WHERE\n",
    "            Freq IS NOT NULL\n",
    "            AND Code IS NOT NULL\n",
    "            AND Date IS NOT NULL\n",
    "            AND Time IS NOT NULL\n",
    "            AND Site IS NOT NULL\n",
    "            AND Power IS NOT NULL\n",
    "            AND Type IS NOT NULL \n",
    "            AND Type = 'LOTEK'\n",
    "            AND Freq IN {tuple(target_frequency_list)}\n",
    "            AND Code IN {tuple(target_code_list)}\n",
    "            -- remove all receiver ids where not matching file name\n",
    "            AND Site = {expected_receiver}\n",
    "        ) TO '{output_parquet_file}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "\n",
    "    # execute query\n",
    "    # try:\n",
    "    duckdb.query(sql)\n",
    "    # except (duckdb.BinderException, duckdb.InvalidInputException) as error:\n",
    "    #     print(f\"ERROR:  Passing import of {input_file}. Please review.\")\n",
    "    #     pass\n",
    "    \n",
    "    return output_parquet_file\n",
    "\n",
    "# def orion_raw_to_parquet(input_file: str,\n",
    "#                          output_directory: str,\n",
    "#                          target_frequency_list: list,\n",
    "#                          target_code_list: list) -> str:\n",
    "#     \"\"\"Create a parquet file for each formatted CSV.\"\"\"\n",
    "    \n",
    "#     # TODO: pass in from outside\n",
    "#     # site_to_receiver = {1: 180, 2: 181, 3: 182, 4: 183, 5: 184, 6: 185, 7: 159, 8: 186, 9: 187, 10: 10, 11: 11, 12: 12, 13: 13}\n",
    "#     site_to_receiver = {1: 128, 2: 117, 3: 118, 4: 119, 5: 120, 6: 116, 7: 121, 8: 122, 9: 123, 10: 124, 11: 125}\n",
    "    \n",
    "#     basename = os.path.basename(input_file)\n",
    "    \n",
    "#     # extract expected site number from file name\n",
    "#     # expected file name format:  \"S9_20220907.csv\"\n",
    "#     #following not working \n",
    "#     #expected_site_number = int(basename.split(\"_\")[0].casefold().split(\"Site\")[-1])\n",
    "#     #try this instead\n",
    "#     expected_site_number = int(basename.split(\"_\")[0].split(\"S\")[-1])\n",
    "    \n",
    "#     # get expected receiver id\n",
    "#     expected_receiver = site_to_receiver[expected_site_number]\n",
    "    \n",
    "#     # extract file name from input file\n",
    "#     file_name = f\"0_ORION_{os.path.splitext(basename)[0]}\"\n",
    "    \n",
    "#     # create output file name\n",
    "#     output_parquet_file = os.path.join(output_directory, f\"{file_name}.parquet\")\n",
    "\n",
    "\n",
    "#     sql = f\"\"\"\n",
    "#     COPY(\n",
    "#         SELECT\n",
    "#             concat(\n",
    "#                 CASE\n",
    "#                     WHEN length(Freq::VARCHAR) = 4\n",
    "#                     THEN Freq::VARCHAR || '000'\n",
    "#                     WHEN length(Freq::VARCHAR) = 5\n",
    "#                     THEN Freq::VARCHAR || '00'        \n",
    "#                     WHEN length(Freq::VARCHAR) = 6\n",
    "#                     THEN Freq::VARCHAR || '0' \n",
    "#                     ELSE Freq::VARCHAR \n",
    "#                 END\n",
    "#                 ,'.'\n",
    "#                 ,CASE\n",
    "#                     WHEN length(Code::VARCHAR) = 1\n",
    "#                     THEN '00' || Code::VARCHAR\n",
    "#                     WHEN length(Code::VARCHAR) = 2\n",
    "#                     THEN '0' || Code::VARCHAR  \n",
    "#                     ELSE Code::VARCHAR\n",
    "#                 END\n",
    "#             ) AS fish_id\n",
    "#             ,(Date + Time) AS date_time\n",
    "#             ,Site AS receiver_id\n",
    "#             ,Power AS signal_power\n",
    "#             ,'{file_name}' AS file_nm\n",
    "#         FROM\n",
    "#             read_csv_auto('{input_file}')\n",
    "#         WHERE\n",
    "#             Freq IS NOT NULL\n",
    "#             AND Code IS NOT NULL\n",
    "#             AND Date IS NOT NULL\n",
    "#             AND Time IS NOT NULL\n",
    "#             AND Site IS NOT NULL\n",
    "#             AND Power IS NOT NULL\n",
    "#             AND Type IS NOT NULL \n",
    "#             AND Type = 'LOTEK'\n",
    "#             AND Freq IN {tuple(target_frequency_list)}\n",
    "#             AND Code IN {tuple(target_code_list)}\n",
    "#             -- remove all receiver ids where not matching file name\n",
    "#             AND Site = {expected_receiver}\n",
    "#         ) TO '{output_parquet_file}' (FORMAT PARQUET);\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # execute query\n",
    "#     try:\n",
    "#         duckdb.query(sql)\n",
    "#     except (duckdb.BinderException, duckdb.InvalidInputException) as error:\n",
    "#         print(f\"ERROR:  Passing import of {input_file}. Please review.\")\n",
    "#         pass\n",
    "    \n",
    "#     return output_parquet_file\n",
    "\n",
    "\n",
    "# def generate_orion_parquet_files(orion_dir: str,\n",
    "#                                  target_frequency_list: List[float],\n",
    "#                                  target_code_list: List[int],\n",
    "#                                  output_directory: str) -> List[str]:\n",
    "#     \"\"\"Generate ORION parquet files for query.\"\"\"\n",
    "    \n",
    "#     # generate the full file list to process\n",
    "#     file_list = tqdm(generate_orion_import_file_list(orion_dir))\n",
    "\n",
    "#     # process files\n",
    "#     processed_files = []\n",
    "#     for i in file_list:\n",
    "\n",
    "#         # convert all whitespace in Orion text files to commas\n",
    "#         raw_csv_file = whitespace_to_csv(i, orion_dir)\n",
    "\n",
    "#         # convert CSV file to parquet format\n",
    "#         parquet_file = orion_raw_to_parquet(input_file=raw_csv_file,\n",
    "#                                             output_directory=output_directory,\n",
    "#                                             target_frequency_list=target_frequency_list,\n",
    "#                                             target_code_list=target_code_list)\n",
    "#         # add processed file to output list\n",
    "#         processed_files.append(parquet_file)\n",
    "    \n",
    "#     return processed_files\n",
    "\n",
    "def generate_orion_parquet_files(orion_dir: str,\n",
    "                                 target_frequency_list: List[float],\n",
    "                                 target_code_list: List[int],\n",
    "                                 output_directory: str,\n",
    "                                 site_to_receiver_dict: dict,\n",
    "                                 target_year=int) -> List[str]:\n",
    "    \"\"\"Generate ORION parquet files for query.\"\"\"\n",
    "    \n",
    "    # generate the full file list to process\n",
    "    file_list = generate_orion_import_file_list(orion_dir)\n",
    "\n",
    "    # process files\n",
    "    processed_files = []\n",
    "    \n",
    "    for i in tqdm(file_list):\n",
    "    \n",
    "        main_name = os.path.splitext(os.path.basename(i))[0]\n",
    "        id = i.split(\"/\")[-1].split(\"_\")[0][1:]\n",
    "     \n",
    "        #if id == '23' or id == '24': \n",
    "         #   continue\n",
    "        orion_name = f'0_ORION_{main_name}.parquet'\n",
    "     \n",
    "        # convert all whitespace in Orion text files to commas\n",
    "        raw_csv_file = whitespace_to_csv(i, orion_dir)\n",
    "    \n",
    "        # convert CSV file to parquet format\n",
    "        parquet_file = orion_raw_to_parquet(input_file=raw_csv_file,\n",
    "                                            output_directory=output_directory,\n",
    "                                            target_frequency_list=target_frequency_list,\n",
    "                                            target_code_list=target_code_list,\n",
    "                                            site_to_receiver_dict=site_to_receiver_dict,\n",
    "                                            target_year=target_year)\n",
    "        processed_files.append(parquet_file)\n",
    "    \n",
    "    return processed_files\n",
    "\n",
    "def get_dst_dates(year):\n",
    "    \"\"\"Gets the start and end dates of DST for a given year.\"\"\"\n",
    "    tz_pacific = pytz.timezone('US/Pacific')  # Use your desired timezone\n",
    "    \n",
    "    # DST starts on the second Sunday of March\n",
    "    dst_start = datetime.datetime(year, 3, 8, 2, 0)\n",
    "    while dst_start.weekday() != 6:  # 6 is Sunday\n",
    "        dst_start += datetime.timedelta(days=1)\n",
    "    dst_start = tz_pacific.localize(dst_start)\n",
    "\n",
    "    # DST ends on the first Sunday of November\n",
    "    dst_end = datetime.datetime(year, 11, 1, 2, 0)\n",
    "    while dst_end.weekday() != 6:\n",
    "        dst_end += datetime.timedelta(days=1)\n",
    "    dst_end = tz_pacific.localize(dst_end)\n",
    "\n",
    "    return dst_start, dst_end\n",
    "\n",
    "def daylight_savings_to_standard(dt, dst_start, dst_end):\n",
    "    \"\"\"Convert a DST timestamp to PST\"\"\"\n",
    "    # set timezone to Pacific\n",
    "    tz_pacific = pytz.timezone('US/Pacific')  \n",
    "\n",
    "    _dt = pd.to_datetime(dt)\n",
    "    _dt = tz_pacific.localize(_dt)\n",
    "    if (_dt >= dst_start) & (_dt <= dst_end):\n",
    "        return _dt - pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        return _dt\n",
    "\n",
    "def mitas_raw_to_parquet(\n",
    "    input_file: str,\n",
    "    output_directory: str,\n",
    "    target_frequency_list: List[float],\n",
    "    target_code_list: List[int],\n",
    "    output_tz = \"US/Pacific\",\n",
    "    target_year= int\n",
    "    ):\n",
    "    \"\"\"Ingest and format an input MITAS file.\"\"\"\n",
    "    \n",
    "    # extract file name from input file\n",
    "    fnm = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    \n",
    "    # create output file name\n",
    "    out_nm = f\"1_MITAS_{fnm}\"\n",
    "    output_parquet_file = os.path.join(output_directory, f\"{out_nm}.parquet\")\n",
    "    \n",
    "    # timezone lookup for daylight savings time (DST) and standard time (ST)\n",
    "    tz_dst = {'decodeTimeUTC-04:00': 'US/Eastern', 'decodeTimeUTC-05:00': 'US/Central', 'decodeTimeUTC-06:00': 'US/Mountain', 'decodeTimeUTC-07:00': 'US/Pacific'}\n",
    "    tz_st = {'decodeTimeUTC-05:00': 'US/Eastern', 'decodeTimeUTC-06:00': 'US/Central', 'decodeTimeUTC-07:00': 'US/Mountain', 'decodeTimeUTC-08:00': 'US/Pacific'}\n",
    "\n",
    "    # list of all possible timestamp column names\n",
    "    all_tz = set(list(tz_dst.keys()) + list(tz_st.keys()))\n",
    "\n",
    "    # get approx download date of mitas file, one day after end date in filename\n",
    "    # TODO: in the future, add download date to mitas filename and use that\n",
    "    fnm_date = fnm.split('_')[-1]\n",
    "    if(fnm==\"20241031_20241102\"):\n",
    "        download_date = datetime.datetime.strptime(fnm_date, '%Y%m%d') + datetime.timedelta(days=2)\n",
    "        download_date = pytz.timezone(output_tz).localize(download_date)\n",
    "    else:\n",
    "        download_date = datetime.datetime.strptime(fnm_date, '%Y%m%d') + datetime.timedelta(days=1)\n",
    "        download_date = pytz.timezone(output_tz).localize(download_date)\n",
    "\n",
    "\n",
    "    # get timestamp column from input file\n",
    "    query = f\"DESCRIBE SELECT * FROM '{input_file}'\"\n",
    "    column_info = duckdb.query(query).fetchall()\n",
    "\n",
    "    try:\n",
    "        timestamp_column = [i[0] for i in column_info if \"decodeTimeUTC\" in i[0]][0]\n",
    "    except IndexError as err:\n",
    "        raise ValueError(\"'decodeTimeUTC column not found in data\")\n",
    "    else:\n",
    "        # print(f'Timestamp column: {timestamp_column}')\n",
    "\n",
    "        if timestamp_column not in all_tz:\n",
    "            raise ValueError(\"None of the expected timestamp columns are present in the data.\")\n",
    "\n",
    "\n",
    "        # determine time zone of data based on UTC offset in header\n",
    "        # use DST lookup if data was downloaded during DST\n",
    "        dst_start, dst_end = get_dst_dates(target_year)\n",
    "        if (download_date > dst_start) & (download_date < dst_end):\n",
    "            mitas_tz = tz_dst[timestamp_column]\n",
    "            is_dst = \"True\"\n",
    "        else:\n",
    "            mitas_tz = tz_st[timestamp_column]\n",
    "            is_dst = \"False\"\n",
    "\n",
    "        # print(f'Input time zone / Is DST:    {mitas_tz} / {is_dst}')\n",
    "\n",
    "        sql = f\"\"\"\n",
    "            COPY(\n",
    "                SELECT\n",
    "                    concat(\n",
    "                        CASE\n",
    "                            WHEN length(frequency::VARCHAR) = 4\n",
    "                            THEN frequency::VARCHAR || '000'\n",
    "                            WHEN length(frequency::VARCHAR) = 5\n",
    "                            THEN frequency::VARCHAR || '00'        \n",
    "                            WHEN length(frequency::VARCHAR) = 6\n",
    "                            THEN frequency::VARCHAR || '0' \n",
    "                            ELSE frequency::VARCHAR \n",
    "                        END\n",
    "                        ,'.'\n",
    "                        ,CASE\n",
    "                            WHEN length(codeNumber::VARCHAR) = 1\n",
    "                            THEN '00' || codeNumber::VARCHAR\n",
    "                            WHEN length(codeNumber::VARCHAR) = 2\n",
    "                            THEN '0' || codeNumber::VARCHAR  \n",
    "                            ELSE codeNumber::VARCHAR\n",
    "                        END\n",
    "                    ) AS fish_id,\n",
    "                    CASE\n",
    "                        WHEN '{is_dst}' = 'True' \n",
    "                        THEN \"{timestamp_column}\" AT TIME ZONE '{mitas_tz}' AT TIME ZONE '{output_tz}' - INTERVAL '1 HOUR'\n",
    "                        ELSE \"{timestamp_column}\" AT TIME ZONE '{mitas_tz}' AT TIME ZONE '{output_tz}'\n",
    "                    END AS date_time,\n",
    "                    ReceiverId AS receiver_id,\n",
    "                    power::INT AS signal_power,\n",
    "                    '{out_nm}' AS file_nm\n",
    "                FROM\n",
    "                    read_csv('{input_file}')\n",
    "                WHERE\n",
    "                    frequency IS NOT NULL\n",
    "                    AND codeNumber IS NOT NULL\n",
    "                    AND '{timestamp_column}' IS NOT NULL\n",
    "                    AND ReceiverId IS NOT NULL\n",
    "                    AND power IS NOT NULL\n",
    "                    AND frequency IN {tuple(target_frequency_list)}\n",
    "                    AND codeNumber IN {tuple(target_code_list)}\n",
    "                ORDER BY date_time\n",
    "                ) TO '{output_parquet_file}' (FORMAT PARQUET);\n",
    "            \"\"\"\n",
    "            \n",
    "        # Execute the main query\n",
    "        duckdb.query(sql)\n",
    "    \n",
    "    return output_parquet_file\n",
    "\n",
    "\n",
    "# def mitas_raw_to_parquet(input_file: str,\n",
    "#                          output_directory: str,\n",
    "#                          target_frequency_list: List[float],\n",
    "#                          target_code_list: List[int],\n",
    "#                          daylight_savings_time_fall: str):\n",
    "#     \"\"\"Ingest and format an input MITAS file.\"\"\"\n",
    "        \n",
    "#     # extract file name from input file\n",
    "#     file_name = f\"1_MITAS_{os.path.splitext(os.path.basename(input_file))[0]}\"\n",
    "    \n",
    "#     # create output file name\n",
    "#     output_parquet_file = os.path.join(output_directory, f\"{file_name}.parquet\")\n",
    "\n",
    "#     sql = f\"\"\"\n",
    "#     COPY(\n",
    "#         SELECT\n",
    "#             concat(\n",
    "#                 CASE\n",
    "#                     WHEN length(frequency::VARCHAR) = 4\n",
    "#                     THEN frequency::VARCHAR || '000'\n",
    "#                     WHEN length(frequency::VARCHAR) = 5\n",
    "#                     THEN frequency::VARCHAR || '00'        \n",
    "#                     WHEN length(frequency::VARCHAR) = 6\n",
    "#                     THEN frequency::VARCHAR || '0' \n",
    "#                     ELSE frequency::VARCHAR \n",
    "#                 END\n",
    "#                 ,'.'\n",
    "#                 ,CASE\n",
    "#                     WHEN length(codeNumber::VARCHAR) = 1\n",
    "#                     THEN '00' || codeNumber::VARCHAR\n",
    "#                     WHEN length(codeNumber::VARCHAR) = 2\n",
    "#                     THEN '0' || codeNumber::VARCHAR  \n",
    "#                     ELSE codeNumber::VARCHAR\n",
    "#                 END\n",
    "#             ) AS fish_id\n",
    "#             ,CASE\n",
    "#                 -- daylight savings time spring adjustment\n",
    "#                 WHEN \"decodeTimeUTC-07:00\" - INTERVAL 3 HOUR >= '{daylight_savings_time_fall}'\n",
    "#                 THEN \"decodeTimeUTC-07:00\" - INTERVAL 4 HOUR\n",
    "#                 ELSE \"decodeTimeUTC-07:00\" - INTERVAL 3 HOUR\n",
    "#             END AS date_time\n",
    "#             ,ReceiverId AS receiver_id\n",
    "#             ,power::INT AS signal_power\n",
    "#             ,'{file_name}' AS file_nm\n",
    "#         FROM\n",
    "#             '{input_file}'\n",
    "#         WHERE\n",
    "#             frequency IS NOT NULL\n",
    "#             AND codeNumber IS NOT NULL\n",
    "#             AND (\"decodeTimeUTC-07:00\" IS NOT NULL OR \"decodeTimeUTC-08:00\" IS NOT NULL)\n",
    "#             AND ReceiverId IS NOT NULL\n",
    "#             AND power IS NOT NULL\n",
    "#             AND frequency IN {tuple(target_frequency_list)}\n",
    "#             AND codeNumber IN {tuple(target_code_list)}\n",
    "#         ) TO '{output_parquet_file}' (FORMAT PARQUET);\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # fire query\n",
    "#     duckdb.query(sql)\n",
    "    \n",
    "#     return output_parquet_file\n",
    "\n",
    "\n",
    "def generate_mitas_parquet_files(mitas_dir: str,\n",
    "                                 target_frequency_list: List[float],\n",
    "                                 target_code_list: List[int],\n",
    "                                 output_directory: str,\n",
    "                                 output_tz = str,\n",
    "                                 target_year=int) -> List[str]:\n",
    "    \"\"\"Generate MITAS parquet files for use in query.\"\"\"\n",
    "    \n",
    "    # get a list of mitas CSV files\n",
    "    mitas_csv_files = glob.glob(os.path.join(mitas_dir, \"*.csv\"))\n",
    "    # print(\"CCCC\")\n",
    "    # print(mitas_csv_files)\n",
    "\n",
    "    processed_files = []\n",
    "    \n",
    "    for j in tqdm(mitas_csv_files):\n",
    "\n",
    "        # convert each file to a parquet file\n",
    "        output_file = mitas_raw_to_parquet(input_file=j, \n",
    "                                           output_directory=output_directory,\n",
    "                                           target_frequency_list=target_frequency_list,\n",
    "                                           target_code_list=target_code_list,\n",
    "                                           output_tz=output_tz,\n",
    "                                           target_year=target_year)\n",
    "        \n",
    "        # add processed file to output list\n",
    "        processed_files.append(output_file)\n",
    "    \n",
    "    return processed_files\n",
    "\n",
    "# def generate_mitas_parquet_files(mitas_dir: str,\n",
    "#                                  target_frequency_list: List[float],\n",
    "#                                  target_code_list: List[int],\n",
    "#                                  output_directory: str,\n",
    "#                                  daylight_savings_time_fall: str) -> List[str]:\n",
    "#     \"\"\"Generate MITAS parquet files for use in query.\"\"\"\n",
    "    \n",
    "#     # get a list of mitas CSV files\n",
    "#     mitas_csv_files = glob.glob(os.path.join(mitas_dir, \"*.csv\"))\n",
    "\n",
    "#     processed_files = []\n",
    "#     for i in tqdm(mitas_csv_files):\n",
    "\n",
    "#         # convert each file to a parquet file\n",
    "#         output_file = mitas_raw_to_parquet(input_file=i, \n",
    "#                                            output_directory=output_directory,\n",
    "#                                            target_frequency_list=target_frequency_list,\n",
    "#                                            target_code_list=target_code_list,\n",
    "#                                            daylight_savings_time_fall=daylight_savings_time_fall)\n",
    "        \n",
    "#         # add processed file to output list\n",
    "#         processed_files.append(output_file)\n",
    "        \n",
    "#     return processed_files\n",
    "\n",
    "\n",
    "def filter_tagged_fish(df, tagging_df):\n",
    "    \"\"\"Only keep fish in tagging file.\"\"\"\n",
    "    \n",
    "    n_records = df.shape[0]\n",
    "\n",
    "    # only keep fish in tagging file\n",
    "    df = df.loc[df[\"fish_id\"].isin(tagging_df[\"fish_id\"].unique())]\n",
    "\n",
    "    n_dropped = n_records - df.shape[0]\n",
    "\n",
    "    print(f\"Dropped {n_dropped} records for fish not in tagging file.\")\n",
    "    \n",
    "    return df \n",
    "\n",
    "\n",
    "def filter_release_time(df, tagging_df, time_buffer_minutes=25):\n",
    "    \"\"\"Only keep records greater than or equal to release time.\"\"\"\n",
    "    \n",
    "    n_records = df.shape[0]\n",
    "\n",
    "    # get a lookup dictionary of release date times from each fish and add time buffer to reduce initial noise\n",
    "    fish_release_time_dict = (tagging_df.set_index(\"fish_id\")[\"tag_release_date_pst\"] + pd.Timedelta(minutes=time_buffer_minutes)).to_dict()\n",
    "\n",
    "    # add field for release time to bound study start\n",
    "    df[\"tag_release_date_pst\"] = df[\"fish_id\"].map(fish_release_time_dict)\n",
    "\n",
    "    # only keep records greater than the fish release time\n",
    "    df = df.loc[df[\"date_time\"] >= df[\"tag_release_date_pst\"]].copy()\n",
    "\n",
    "    df.drop(columns=[\"tag_release_date_pst\"], inplace=True)\n",
    "\n",
    "    n_dropped = n_records - df.shape[0]\n",
    "\n",
    "    print(f\"Dropped {n_dropped} records for detections before release time.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_study_period(df, \n",
    "                        end_date_time):\n",
    "    \"\"\"Only keep records that span through the study period.\"\"\"\n",
    "    \n",
    "    n_records = df.shape[0]\n",
    "\n",
    "    # only keep records that account for tag life\n",
    "    df = df.loc[df[\"date_time\"] <= end_date_time]\n",
    "\n",
    "    n_dropped = n_records - df.shape[0]\n",
    "\n",
    "    print(f\"Dropped {n_dropped} records exceeding study period date and time.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_tag_life(df: pd.DataFrame, \n",
    "                    tagging_df: pd.DataFrame, \n",
    "                    max_tag_life_days: float) -> pd.DataFrame:\n",
    "    \"\"\"Only keep records that span through tag life.\"\"\"\n",
    "    \n",
    "    n_records = df.shape[0]\n",
    "    \n",
    "    # create a tag expiration date\n",
    "    tagging_df[\"tag_expire_dt\"] = tagging_df[\"tag_activation_date_pst\"] + pd.Timedelta(days=max_tag_life_days)\n",
    "    \n",
    "    # create a dictionary of tax expiration datetime\n",
    "    tag_expire_dict = tagging_df.set_index(\"fish_id\")[\"tag_expire_dt\"].to_dict()\n",
    "    \n",
    "    # add the expiration date to the input data frame\n",
    "    df[\"tag_expire_dt\"] = df[\"fish_id\"].map(tag_expire_dict)\n",
    "\n",
    "    # only keep records that account for tag life\n",
    "    df = df.loc[df[\"date_time\"] <= df[\"tag_expire_dt\"]]\n",
    "\n",
    "    n_dropped = n_records - df.shape[0]\n",
    "\n",
    "    print(f\"Dropped {n_dropped} records where detection datetime exceeded tag life.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_drop_duplicate_detections(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicate detections by keeping the first files which are Orion files.\"\"\"\n",
    "\n",
    "    n_records = df.shape[0]\n",
    "\n",
    "    # drop duplicates by keeping \"first\" which are the orion files\n",
    "    df.drop_duplicates(subset=[\"fish_id\", \"date_time\", \"site_number\", \"signal_power\"], \n",
    "                       keep=\"first\", \n",
    "                       inplace=True)\n",
    "\n",
    "    n_dropped = n_records - df.shape[0]\n",
    "\n",
    "    print(f\"Dropped {n_dropped} duplicate MITAS and ORION records.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_raw_data(target_fish_id: str,\n",
    "                    glob_path: str,\n",
    "                    reciever_to_detect_site_dict: dict,\n",
    "                    receiver_to_site_number_dict: dict,\n",
    "                    tagging_df: pd.DataFrame,\n",
    "                    project_end_date: str,\n",
    "                    max_tag_life_days: float) -> pd.DataFrame:\n",
    "    \"\"\"Apply filters to raw data and add detection site and site number.\n",
    "    \n",
    "    Filter 1:  only expected fish in the data \n",
    "    Filter 2:  ensures the fish times are bound by release datetime\n",
    "    Filter 3:  ensures that only detections that fall into the tag life window are considered\n",
    "    Filter 4:  ensure that only detections that fall into the tag life window are considered\n",
    "    Filter 5:  drop duplicates occurring in MITAS and ORION; keep ORION by default\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \n",
    "        '{glob_path}'\n",
    "    WHERE\n",
    "        fish_id = '{target_fish_id}';\n",
    "    \"\"\"\n",
    "    \n",
    "    df = duckdb.query(sql).df()\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        print(f\"WARNING:  There were no valid detections for fish_id:  '{target_fish_id}'\")\n",
    "        print(f\"WARNING:  Output file will not be created.\")\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        # add detect site\n",
    "        df[\"detect_site\"] = df[\"receiver_id\"].map(reciever_to_detect_site_dict)\n",
    "\n",
    "        # change receiver id to site number\n",
    "        df[\"site_number\"] = df[\"receiver_id\"].map(receiver_to_site_number_dict)\n",
    "\n",
    "        # ensure that only expected fish are in the data\n",
    "        df = filter_tagged_fish(df, tagging_df)\n",
    "\n",
    "        # ensure that fish times are bound by release time\n",
    "        df = filter_release_time(df, tagging_df)\n",
    "\n",
    "        # ensure that only detections that fall into the study period window\n",
    "        df = filter_study_period(df,\n",
    "                                 end_date_time=project_end_date)\n",
    "        \n",
    "        # ensure that only detections that fall into the tag life window are considered\n",
    "        df = filter_tag_life(df,\n",
    "                             tagging_df,\n",
    "                             max_tag_life_days=max_tag_life_days)\n",
    "\n",
    "\n",
    "        # drop duplicates by keeping \"first\" which are the orion files\n",
    "        df = filter_drop_duplicate_detections(df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_lag_lead_records(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create lag and lead records for the target fish to show what happened before\n",
    "    and after the target.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # sort data frame\n",
    "    df.sort_values(by=[\"fish_id\", \"detect_site\", \"date_time\"], inplace=True)\n",
    "\n",
    "    # reindex dataset\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # lag\n",
    "    lag_df = df.shift(periods=1)[[\"fish_id\", \"detect_site\", \"date_time\"]]\n",
    "\n",
    "    # lead\n",
    "    lead_df = df.shift(periods=-1)[[\"fish_id\", \"detect_site\", \"date_time\"]]\n",
    "\n",
    "    # add to main data frame\n",
    "    df[\"lag_fish_id\"] = lag_df[\"fish_id\"]\n",
    "    df[\"lag_detect_site\"] = lag_df[\"detect_site\"]\n",
    "    df[\"lag_date_time\"] = lag_df[\"date_time\"]\n",
    "    df[\"lead_fish_id\"] = lead_df[\"fish_id\"]\n",
    "    df[\"lead_detect_site\"] = lead_df[\"detect_site\"]\n",
    "    df[\"lead_date_time\"] = lead_df[\"date_time\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_events(df, hits, seconds, detection_sites):\n",
    "    \n",
    "    df = df.loc[df[\"detect_site\"].isin(detection_sites)].copy()\n",
    "\n",
    "    # create timedelta field in seconds; set\n",
    "    df['time_from_previous_hit'] = np.where(\n",
    "                                    (df.fish_id == df.lag_fish_id) & (df.detect_site == df.lag_detect_site),\n",
    "                                    (df.date_time - df.lag_date_time).fillna(pd.Timedelta('0 days')).values.view('<i8')/10**9,\n",
    "                                    -1)\n",
    "\n",
    "    # create block_id for each event where hits are no more than time threshold seconds apart\n",
    "    df['block_id'] = ((df.time_from_previous_hit >= seconds) | (df.time_from_previous_hit < 0)).astype(int).cumsum()\n",
    "\n",
    "    # get hit count of each block\n",
    "    df['block_count'] = df.groupby(['block_id'])['block_id'].transform('count')\n",
    "\n",
    "    # remove unneeded columns\n",
    "    drop_cols = ['time_from_previous_hit', 'lead_fish_id', 'lag_fish_id', 'lead_detect_site', 'lag_detect_site',\n",
    "                 'lead_date_time', 'lag_date_time']\n",
    "\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    # only keep event blocks that meet the hits per block threshold\n",
    "    df = df[df['block_count'] >= hits]\n",
    "    \n",
    "    # add in hits per sec claus\n",
    "    df[\"grouping\"] = f\"{hits}_{seconds}\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def build_first_last_detection(df, detect_site_to_abbrev_dict):\n",
    "    \"\"\"Generate first and last detections at each detect site.\"\"\"\n",
    "\n",
    "    # generate last event time for each detection site\n",
    "    events_last = df.groupby([\"fish_id\", \"grouping\", \"detect_site\"])[\"date_time\"].max().reset_index()\n",
    "    events_last[\"site_abbrev\"] = events_last[\"detect_site\"].map(detect_site_to_abbrev_dict).str.lower() + \"_l\"\n",
    "\n",
    "    # generate first event time for each detection site\n",
    "    events_first = df.groupby([\"fish_id\", \"grouping\", \"detect_site\"])[\"date_time\"].min().reset_index()\n",
    "    events_first[\"site_abbrev\"] = events_first[\"detect_site\"].map(detect_site_to_abbrev_dict).str.lower() + \"_f\"\n",
    "\n",
    "    # combine\n",
    "    events_first_last = pd.concat([events_first, events_last])\n",
    "\n",
    "    return events_first_last.sort_values(by=\"detect_site\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_fish_heatmap(fish_id: str,\n",
    "                      df: pd.DataFrame,\n",
    "                      output_directory: str,\n",
    "                      detect_plot_name_dict: dict,\n",
    "                      site_plot_name_dict: dict,\n",
    "                      time_aggregation: str = \"min\",\n",
    "                      plot_field: str = \"detect_site\",\n",
    "                      plot_type: str = \"power\",\n",
    "                      figsize: tuple = (30, 8),\n",
    "                      title: str = \"\",\n",
    "                      file_name_suffix: str = \"\",\n",
    "                      dpi: int = 80):\n",
    "    \n",
    "    if plot_field == \"site_number\":\n",
    "        map_plot_name_dict = site_plot_name_dict\n",
    "\n",
    "        \n",
    "    else:\n",
    "        map_plot_name_dict = detect_plot_name_dict\n",
    "\n",
    "    \n",
    "    beacon_df[f\"{plot_field}_name\"] = beacon_df[plot_field].map(map_plot_name_dict)\n",
    "\n",
    "    df[f\"{plot_field}_name\"] = df[plot_field].map(map_plot_name_dict)\n",
    "\n",
    "    df[\"date\"] = df[\"date_time\"].dt.floor(time_aggregation).dt.strftime('%m-%d %H:%M:00')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if plot_type == \"detection\":\n",
    "        \n",
    "        #plot_detections = df.groupby([f\"{plot_field}_name\", \"date\"]).count().reset_index().pivot(f\"{plot_field}_name\", \"date\", \"fish_id\")\n",
    "        plot_detections = df.groupby([f\"{plot_field}_name\", \"date\"]).count().reset_index().pivot(index=f\"{plot_field}_name\", columns=\"date\", values=\"fish_id\")\n",
    "\n",
    "        g = sns.heatmap(plot_detections, annot=False, cmap=\"vlag\", ax=ax) #linewidths=0.5, ax=ax)\n",
    "        g.set_title(title)\n",
    "        \n",
    "    elif plot_type == \"power\":\n",
    "        \n",
    "        # more negative power is weaker; closer to 0 is more powerful\n",
    "        #plot_power = df.groupby([f\"{plot_field}_name\", \"date\"]).max().reset_index().pivot(f\"{plot_field}_name\", \"date\", \"signal_power\")\n",
    "        plot_power = df.groupby([f\"{plot_field}_name\", \"date\"]).max().reset_index().pivot(index=f\"{plot_field}_name\",columns=\"date\", values=\"signal_power\")\n",
    "\n",
    "        g = sns.heatmap(plot_power, annot=False, cmap=\"vlag\", ax=ax) #linewidths=0.5, ax=ax)\n",
    "        g.set_title(title)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Option for 'plot_type' = '{plot_type}' is not available.  Choose either 'detection' or 'power'.\")\n",
    "    \n",
    "    figure = g.get_figure()\n",
    "    figure.savefig(os.path.join(output_directory, f\"{fish_id}_{file_name_suffix}.png\"), dpi=dpi, bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate(fish_id: str, \n",
    "             output_directory: str,\n",
    "             parquet_raw_dir: str,\n",
    "             travel_time_template_dtypes: dict,\n",
    "             reciever_to_detect_site_dict: dict,\n",
    "             receiver_to_site_number_dict: dict,\n",
    "             signal_power_threshold_dict: dict,\n",
    "             detect_site_to_abbrev_dict: dict,\n",
    "             detection_site_abbrev_list: list,\n",
    "             detect_plot_name_dict: dict,\n",
    "             site_plot_name_dict: dict,\n",
    "             tagging_df: pd.DataFrame,\n",
    "             beacon_df: pd.DataFrame,\n",
    "             project_end_date: str,\n",
    "             max_tag_life_days: int,\n",
    "             use_events: bool, \n",
    "             consider_dam_operations: bool,\n",
    "             output_plot_directory: str,\n",
    "             simulation: str,\n",
    "             generate_plots: bool = True,\n",
    "             plot_size: tuple = (60, 7),\n",
    "             write_clean_data: bool = False,\n",
    "             generate_clean_plots: bool = False):\n",
    "    \n",
    "    \n",
    "    # get path to glob all raw parquet files\n",
    "    glob_path = os.path.join(parquet_raw_dir, \"*.parquet\")\n",
    "\n",
    "\n",
    "    # process    \n",
    "    df = filter_raw_data(fish_id,\n",
    "                         glob_path,\n",
    "                         reciever_to_detect_site_dict=reciever_to_detect_site_dict,\n",
    "                         receiver_to_site_number_dict=receiver_to_site_number_dict,\n",
    "                         tagging_df=tagging_df,\n",
    "                         project_end_date=project_end_date,\n",
    "                         max_tag_life_days=max_tag_life_days)\n",
    "    if write_clean_data:\n",
    "        df.sort_values(by=\"date_time\").to_csv(os.path.join(output_directory, f\"{simulation}/clean/clean_by_fish_id/{fish_id}.csv\"), index=False)\n",
    "    \n",
    "    if generate_clean_plots:\n",
    "    \n",
    "        # set level of time aggregation for plots\n",
    "        if df.shape[0] <= 20000:\n",
    "            time_aggregation = \"min\"\n",
    "            time_designation = \"minute\"\n",
    "        else:\n",
    "            time_aggregation = \"H\"\n",
    "            time_designation = \"hour\"\n",
    "\n",
    "        print(\"Producing clean plots...\")\n",
    "\n",
    "        # plot heatmap of detections  \n",
    "        plot_fish_heatmap(fish_id=fish_id,\n",
    "                          df=df,\n",
    "                          output_directory=os.path.join(output_directory, f\"{simulation}/clean/clean_plots\"),\n",
    "                          detect_plot_name_dict=detect_plot_name_dict,\n",
    "                          site_plot_name_dict=site_plot_name_dict,\n",
    "                          time_aggregation=time_aggregation,\n",
    "                          plot_field=\"site_number\",\n",
    "                          plot_type=\"detection\",\n",
    "                          figsize=plot_size,\n",
    "                          title=f\"Clean detections by site number aggregated to {time_designation}:  {fish_id}\",\n",
    "                          file_name_suffix=\"event_detections-by-site_number\")\n",
    "\n",
    "        # plot heatmap of signal power\n",
    "        plot_fish_heatmap(fish_id=fish_id,\n",
    "                          df=df,\n",
    "                          output_directory=os.path.join(output_directory, f\"{simulation}/clean/clean_plots\"),\n",
    "                          detect_plot_name_dict=detect_plot_name_dict,\n",
    "                          site_plot_name_dict=site_plot_name_dict,\n",
    "                          time_aggregation=time_aggregation,\n",
    "                          plot_field=\"site_number\",\n",
    "                          plot_type=\"power\",\n",
    "                          figsize=plot_size,\n",
    "                          title=f\"Clean signal power per detecion by site number aggregated to {time_designation}:  {fish_id}\",\n",
    "                          file_name_suffix=\"event_signal_power-by-site_number\")\n",
    "\n",
    "\n",
    "        # plot heatmap of detections  \n",
    "        plot_fish_heatmap(fish_id=fish_id,\n",
    "                          df=df,\n",
    "                          output_directory=os.path.join(output_directory, f\"{simulation}/clean/clean_plots\"),\n",
    "                          detect_plot_name_dict=detect_plot_name_dict,\n",
    "                          site_plot_name_dict=site_plot_name_dict,\n",
    "                          time_aggregation=time_aggregation,\n",
    "                          plot_field=\"detect_site\",\n",
    "                          plot_type=\"detection\",\n",
    "                          figsize=plot_size,\n",
    "                          title=f\"Clean detections by detecion site aggregated to {time_designation}:  {fish_id}\",\n",
    "                          file_name_suffix=\"event_detections-by-detect_site\")\n",
    "\n",
    "        # plot heatmap of signal power\n",
    "        plot_fish_heatmap(fish_id=fish_id,\n",
    "                          df=df,\n",
    "                          output_directory=os.path.join(output_directory, f\"{simulation}/clean/clean_plots\"),\n",
    "                          detect_plot_name_dict=detect_plot_name_dict,\n",
    "                          site_plot_name_dict=site_plot_name_dict,\n",
    "                          time_aggregation=time_aggregation,\n",
    "                          plot_field=\"detect_site\",\n",
    "                          plot_type=\"power\",\n",
    "                          figsize=plot_size,\n",
    "                          title=f\"Clean signal power per detection by detect site aggregated to {time_designation}:  {fish_id}\",\n",
    "                          file_name_suffix=\"event_signal_power-by-detect_site\")\n",
    "\n",
    "    if use_events:\n",
    "\n",
    "        # create lag and lead records\n",
    "        df = generate_lag_lead_records(df)\n",
    "\n",
    "        # get a list of detection sites per hits per second condition\n",
    "        cond_one = beacon_df.loc[beacon_df[\"hits_seconds_run\"] == \"3_60\"][\"detect_site\"].to_list()\n",
    "        cond_two = beacon_df.loc[beacon_df[\"hits_seconds_run\"] == \"2_120\"][\"detect_site\"].to_list()\n",
    "\n",
    "        # create events per condition\n",
    "        dfa = create_events(df, hits=3, seconds=60, detection_sites=cond_one)\n",
    "        dfb = create_events(df, hits=2, seconds=120, detection_sites=cond_two)\n",
    "\n",
    "        # merge output\n",
    "        events_nops = pd.concat([dfa, dfb])\n",
    "\n",
    "    else:\n",
    "\n",
    "        # if not calculating events\n",
    "        events_nops = df.copy()\n",
    "\n",
    "        events_nops[\"grouping\"] = None\n",
    "\n",
    "    # only keep event detections that are greater than or equal to the site specific thresholds for signal power\n",
    "    events_nops = events_nops.loc[events_nops[\"signal_power\"] >= events_nops[\"site_number\"].map(signal_power_threshold_dict)]\n",
    "\n",
    "    # generate first and last detections per detection site\n",
    "    events_nops_tt = build_first_last_detection(events_nops, detect_site_to_abbrev_dict)\n",
    "\n",
    "    events_ops = events_nops\n",
    "    events_ops_tt = events_nops_tt    \n",
    "    \n",
    "    if events_ops.shape[0] == 0:\n",
    "        print(f\"No events available.\")\n",
    "        generate_plots = False\n",
    "        \n",
    "    if generate_plots:\n",
    "        print(\"Producing events plots...\")\n",
    "        \n",
    "        # set level of time aggregation for plots\n",
    "        if events_ops.shape[0] <= 20000:\n",
    "            time_aggregation = \"min\"\n",
    "            time_designation = \"minute\"\n",
    "        else:\n",
    "            time_aggregation = \"H\"\n",
    "            time_designation = \"hour\"\n",
    "\n",
    "        # plot heatmap of detections  \n",
    "        plot_fish_heatmap(fish_id=fish_id,\n",
    "                          df=events_ops,\n",
    "                          output_directory=output_plot_directory,\n",
    "                          detect_plot_name_dict=detect_plot_name_dict,\n",
    "                          site_plot_name_dict=site_plot_name_dict,\n",
    "                          time_aggregation=time_aggregation,\n",
    "                          plot_field=\"site_number\",\n",
    "                          plot_type=\"detection\",\n",
    "                          figsize=plot_size,\n",
    "                          title=f\"Event detections by site number aggregated to {time_designation}:  {fish_id}\",\n",
    "                          file_name_suffix=\"event_detections-by-site_number\")\n",
    "\n",
    "        # plot heatmap of signal power\n",
    "        plot_fish_heatmap(fish_id=fish_id,\n",
    "                          df=events_ops,\n",
    "                          output_directory=output_plot_directory,\n",
    "                          detect_plot_name_dict=detect_plot_name_dict,\n",
    "                          site_plot_name_dict=site_plot_name_dict,\n",
    "                          time_aggregation=time_aggregation,\n",
    "                          plot_field=\"site_number\",\n",
    "                          plot_type=\"power\",\n",
    "                          figsize=plot_size,\n",
    "                          title=f\"Event signal power per detecion by site number aggregated to {time_designation}:  {fish_id}\",\n",
    "                          file_name_suffix=\"event_signal_power-by-site_number\")\n",
    "\n",
    "        # plot heatmap of detections  \n",
    "        plot_fish_heatmap(fish_id=fish_id,\n",
    "                          df=events_ops,\n",
    "                          output_directory=output_plot_directory,\n",
    "                          detect_plot_name_dict=detect_plot_name_dict,\n",
    "                          site_plot_name_dict=site_plot_name_dict,\n",
    "                          time_aggregation=time_aggregation,\n",
    "                          plot_field=\"detect_site\",\n",
    "                          plot_type=\"detection\",\n",
    "                          figsize=plot_size,\n",
    "                          title=f\"Event detections by detecion site aggregated to {time_designation}:  {fish_id}\",\n",
    "                          file_name_suffix=\"event_detections-by-detect_site\")\n",
    "\n",
    "        # plot heatmap of signal power\n",
    "        plot_fish_heatmap(fish_id=fish_id,\n",
    "                          df=events_ops,\n",
    "                          output_directory=output_plot_directory,\n",
    "                          detect_plot_name_dict=detect_plot_name_dict,\n",
    "                          site_plot_name_dict=site_plot_name_dict,\n",
    "                          time_aggregation=time_aggregation,\n",
    "                          plot_field=\"detect_site\",\n",
    "                          plot_type=\"power\",\n",
    "                          figsize=plot_size,\n",
    "                          title=f\"Event signal power per detection by detect site aggregated to {time_designation}:  {fish_id}\",\n",
    "                          file_name_suffix=\"event_signal_power-by-detect_site\")\n",
    "\n",
    "    # build a dictionary of field with empty data\n",
    "    travel_time_template_dict = {i: [] for i in travel_time_template_dtypes.keys()}\n",
    "\n",
    "    # generate travel time template\n",
    "    travel_time_template = pd.DataFrame(travel_time_template_dict).astype(travel_time_template_dtypes)\n",
    "\n",
    "    # add fish ids to the travel time file\n",
    "    travel_time_template[\"fish_id\"] = tagging_df.index\n",
    "\n",
    "    # set template site designation to 0 \n",
    "    travel_time_template[detection_site_abbrev_list] = 0\n",
    "\n",
    "    # generate a list of detection site first and last columns in order\n",
    "    detection_site_time_columns = []\n",
    "    for i in detection_site_abbrev_list:\n",
    "        detection_site_time_columns.append(f\"{i}_f\")\n",
    "        detection_site_time_columns.append(f\"{i}_l\")\n",
    "\n",
    "    # sort by fish id\n",
    "    travel_time_template.sort_values(by=[\"fish_id\"], inplace=True)\n",
    "\n",
    "    # set index to fish id\n",
    "    travel_time_template.set_index(\"fish_id\", inplace=True)\n",
    "    \n",
    "\n",
    "    if events_ops.shape[0] > 0:\n",
    "        events_ops_tt_layout = pd.pivot_table(events_ops_tt,\n",
    "                                               values=\"date_time\",\n",
    "                                               index=[\"fish_id\"],\n",
    "                                               columns=[\"site_abbrev\"]).rename_axis(None, axis=1).reset_index()\n",
    "    else:\n",
    "        events_ops_tt_layout = pd.DataFrame({\"fish_id\": [fish_id], \"date_time\": [pd.NaT]})\n",
    "    \n",
    "    events_ops_tt_layout[\"pit_code\"] = events_ops_tt_layout[\"fish_id\"].map(pit_tag_dict)\n",
    "    events_ops_tt_layout[\"srr\"] = events_ops_tt_layout[\"fish_id\"].map(srr_dict)\n",
    "    events_ops_tt_layout[\"rel_name\"] = events_ops_tt_layout[\"fish_id\"].map(release_name_dict)\n",
    "    events_ops_tt_layout[\"lot\"] = events_ops_tt_layout[\"fish_id\"].map(lot_dict)\n",
    "    events_ops_tt_layout[\"act_datetime\"] = events_ops_tt_layout[\"fish_id\"].map(activation_dict)\n",
    "    events_ops_tt_layout[\"release_datetime\"] = events_ops_tt_layout[\"fish_id\"].map(release_dict)\n",
    "    events_ops_tt_layout[\"mort_xlat\"] = events_ops_tt_layout[\"fish_id\"].map(mort_dict)\n",
    "\n",
    "    events_ops_tt_layout[\"hole\"] = \"NA\"\n",
    "    events_ops_tt_layout[\"subhole\"] = \"NA\"\n",
    "    events_ops_tt_layout[\"rel_weir_time_s\"] = 0\n",
    "    events_ops_tt_layout[\"pool_stage\"] = \"\"\n",
    "    events_ops_tt_layout[\"release_stage\"] = \"NA\"\n",
    "    events_ops_tt_layout[\"censor\"] = \"\"\n",
    "    events_ops_tt_layout[\"altered\"] = \"\"\n",
    "    events_ops_tt_layout[\"comments\"] = \"\"\n",
    "    \n",
    "    for i in detection_site_abbrev_list:\n",
    "        events_ops_tt_layout[i] = 0\n",
    "        \n",
    "        first = f\"{i}_f\"\n",
    "        last = f\"{i}_l\"\n",
    "        \n",
    "        if first not in events_ops_tt_layout:\n",
    "            events_ops_tt_layout[first] = pd.NaT\n",
    "        \n",
    "        if last not in events_ops_tt_layout:\n",
    "            events_ops_tt_layout[last] = pd.NaT\n",
    "\n",
    "    events_ops_tt_layout = events_ops_tt_layout[list(travel_time_template.reset_index().columns)].set_index(\"fish_id\")\n",
    "\n",
    "    # update template fields for each detection site where an event was logged regardless of whether or not the fish passed the site\n",
    "    x = events_ops.set_index(\"fish_id\")[\"detect_site\"].map({i: detect_site_to_abbrev_dict[i].lower() for i in detect_site_to_abbrev_dict.keys()}).reset_index()\n",
    "\n",
    "    x[\"detected\"] = 1\n",
    "\n",
    "    detect_designation = pd.pivot_table(x, \n",
    "                                        values=\"detected\", \n",
    "                                        index=\"fish_id\", \n",
    "                                        columns=\"detect_site\", \n",
    "                                        fill_value=0).rename_axis(None, axis=1)\n",
    "\n",
    "    # update the template data frame with the new data\n",
    "    events_ops_tt_layout.update(detect_designation)\n",
    "\n",
    "    travel_time_df = events_ops_tt_layout.copy()\n",
    "\n",
    "    # calculate the releative weir time in seconds\n",
    "    travel_time_df[\"rel_weir_time_s\"] = (travel_time_df['tw1_l'] - travel_time_df['tw1_f']).dt.seconds.fillna(0)\n",
    "    \n",
    "    # keep the following event fields\n",
    "    events_keep_fields = ['fish_id', 'date_time', 'receiver_id', 'signal_power', 'file_nm', \n",
    "                          'detect_site', 'site_number', 'block_id', 'block_count']\n",
    "\n",
    "    return travel_time_df, events_ops[events_keep_fields]\n",
    "\n",
    "\n",
    "def process_parallel(fish_id,\n",
    "                     output_directory,\n",
    "                     parquet_raw_dir,\n",
    "                      travel_time_template_dtypes,\n",
    "                      reciever_to_detect_site_dict,\n",
    "                      receiver_to_site_number_dict,\n",
    "                      signal_power_threshold_dict,\n",
    "                      detect_site_to_abbrev_dict,\n",
    "                      detection_site_abbrev_list,\n",
    "                      detect_plot_name_dict,\n",
    "                      site_plot_name_dict,\n",
    "                      simulation,\n",
    "                      tagging_df,\n",
    "                      beacon_df,\n",
    "                      project_end_date,\n",
    "                      max_tag_life_days,\n",
    "                      use_events, \n",
    "                      consider_dam_operations,\n",
    "                      output_plot_directory,\n",
    "                      generate_event_plots,\n",
    "                      write_clean_data,\n",
    "                      generate_clean_plots):\n",
    "    \"\"\"Wrap generate function to allow parallel processing of each fish.\"\"\"\n",
    "    \n",
    "    ttdf, evdf = generate(fish_id=fish_id,\n",
    "                          output_directory=output_directory,\n",
    "                          parquet_raw_dir=parquet_raw_dir,\n",
    "                          travel_time_template_dtypes=travel_time_template_dtypes,\n",
    "                          reciever_to_detect_site_dict=reciever_to_detect_site_dict,\n",
    "                          receiver_to_site_number_dict=receiver_to_site_number_dict,\n",
    "                          signal_power_threshold_dict=site_to_power_percentile_dict,\n",
    "                          detect_site_to_abbrev_dict=detect_site_to_abbrev_dict,\n",
    "                          detection_site_abbrev_list=detection_site_abbrev_list,\n",
    "                          detect_plot_name_dict=detect_plot_name_dict,\n",
    "                          site_plot_name_dict=site_plot_name_dict,\n",
    "                          simulation=simulation,\n",
    "                          tagging_df=tagging_df,\n",
    "                          beacon_df=beacon_df,\n",
    "                          project_end_date=project_end_date,\n",
    "                          max_tag_life_days=max_tag_life_days,\n",
    "                          use_events=use_events, \n",
    "                          consider_dam_operations=consider_dam_operations,\n",
    "                          output_plot_directory=output_plot_directory,\n",
    "                          generate_plots=generate_event_plots,\n",
    "                          write_clean_data=write_clean_data,\n",
    "                          generate_clean_plots=generate_clean_plots)\n",
    "\n",
    "    # save event detection by fish id CSV file\n",
    "    print(\"Writing events by fish id...\")\n",
    "    evdf.sort_values(by=\"date_time\").to_csv(os.path.join(output_directory, f\"{simulation}/events/events_by_fish_id/{fish_id}.csv\"), index=False)\n",
    "\n",
    "    return ttdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DST starts: 2024-03-10 02:00:00-08:00\n",
      "DST ends: 2024-11-03 02:00:00-08:00\n"
     ]
    }
   ],
   "source": [
    "# project directory\n",
    "root_dir = \"/Users/rech095/Documents/Toutle/toutle_fall_2024/\"\n",
    "\n",
    "# data directory holding raw data\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "\n",
    "# mitas raw data directory\n",
    "mitas_dir = os.path.join(data_dir, \"mitas\")\n",
    "mitas_dir = mitas_dir.replace(\"\\\\\", \"/\")\n",
    "\n",
    "# orion raw data directory\n",
    "orion_dir = os.path.join(data_dir, \"orion\")\n",
    "orion_dir = orion_dir.replace(\"\\\\\", \"/\")\n",
    "\n",
    "# output directory\n",
    "output_dir = os.path.join(data_dir, \"outputs\")\n",
    "output_dir = output_dir.replace(\"\\\\\", \"/\")\n",
    "\n",
    "# directory to hold formatted raw parquet files for query\n",
    "parquet_raw_dir = os.path.join(data_dir, \"parquet_raw_data\")\n",
    "parquet_raw_dir = parquet_raw_dir.replace(\"\\\\\", \"/\")\n",
    "\n",
    "# project start date\n",
    "project_start_date = \"2024-10-21 08:00:00\"\n",
    "\n",
    "# project end date\n",
    "project_end_date = \"2024-12-11 14:00:00\"\n",
    "\n",
    "# daylight savings time spring start\n",
    "daylight_savings_time_fall = \"2024-11-03 02:00:00\"\n",
    "\n",
    "# study year\n",
    "target_year = 2024\n",
    "\n",
    "# daylight savings time spring start\n",
    "# Get DST dates for target year\n",
    "dst_start, dst_end = get_dst_dates(target_year)\n",
    "print(\"DST starts:\", dst_start)\n",
    "print(\"DST ends:\", dst_end)\n",
    "daylight_savings_time_spring = dst_start.strftime(\"%Y-%m-%d %H:%M:%S\")# \"2024-03-10 02:00:00\"\n",
    "\n",
    "# maximum tag life days\n",
    "max_tag_life_days = 93.4\n",
    "\n",
    "# supporting files\n",
    "beacon_file = os.path.join(root_dir, \"data\", \"load_db\", \"tbl_beacons_toutle_fall_2024.csv\")\n",
    "tagging_file = os.path.join(root_dir, \"data\", \"load_db\", \"acttagrel_toutle_2024.xlsx\")\n",
    "# dam_ops_file = os.path.join(root_dir, \"data\", \"load_db\", \"Hourly_dam_ops_foster_2022_final_091522.csv\")\n",
    "\n",
    "# for reproducibility of any stochastic process\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for use with plot_field = detect_site\n",
    "detect_plot_name_dict = {1: '01_HAT',\n",
    "                         2: '02_TW1',\n",
    "                         3: '03_TW2',\n",
    "                         4: '04_EN1',\n",
    "                         5: '05_EN2',\n",
    "                         6: '06_EN3',\n",
    "                         7: '07_LA1',\n",
    "                         8: '08_LA2',\n",
    "                         9: '09_LA3',\n",
    "                        10: '10_LA4',\n",
    "                        11: '11_PS1'}\n",
    "\n",
    "# for use with plot field = site_number\n",
    "site_plot_name_dict= {1: \"order-01_HAT\",\n",
    "                      2: \"order-02_TW1\",\n",
    "                      3: \"order-03_TW2\",\n",
    "                      4: \"order-04_EN1\",\n",
    "                      5: \"order-05_EN2\",\n",
    "                      6: \"order-06_EN3\",\n",
    "                      7: \"order-07_LA1\",\n",
    "                      8: \"order-08_LA2\",\n",
    "                      9: \"order-09_LA3\",\n",
    "                     10: \"order-10_LA4\",\n",
    "                     11: \"order-11_PS1\"}\n",
    "\n",
    "# list of detection sites in travel order\n",
    "detection_site_abbrev_list = ['hat', 'tw1', 'tw2', 'en1', 'en2', 'en3', 'la1', 'la2', 'la3', 'la4', 'ps1']\n",
    "\n",
    "# dictionary of detection sites to travel time file abbreviation\n",
    "detect_site_to_abbrev_dict = {1: 'hat',\n",
    "                              2: 'tw1',\n",
    "                              3: 'tw2',\n",
    "                              4: 'en1',\n",
    "                              5: 'en2',\n",
    "                              6: 'en3',\n",
    "                              7: 'la1',\n",
    "                              8: 'la2',\n",
    "                              9: 'la3',\n",
    "                              10: 'la4',\n",
    "                              11: 'ps1'}\n",
    "\n",
    "# dictionary of data types for the travel time data frame\n",
    "travel_time_template_dtypes = {\n",
    "    \"fish_id\": str,\n",
    "    \"pit_code\": str,\n",
    "    \"rel_name\": str,\n",
    "    \"lot\": int,\n",
    "    \"srr\": str,\n",
    "    \"act_datetime\": \"datetime64[ns]\",\n",
    "    \"release_datetime\": \"datetime64[ns]\",\n",
    "    \"hole\": str,\n",
    "    \"subhole\": str,\n",
    "    \"hat\": int,\n",
    "    \"tw1\": int,\n",
    "    \"tw2\": int,\n",
    "    \"en1\": int,\n",
    "    \"en2\": int,\n",
    "    \"en3\": int,\n",
    "    \"la1\": int,\n",
    "    \"la2\": int,\n",
    "    \"la3\": int,\n",
    "    \"la4\": int,\n",
    "    \"ps1\": int,\n",
    "    \"hat_f\": \"datetime64[ns]\",\n",
    "    \"hat_l\": \"datetime64[ns]\",\n",
    "    \"tw1_f\": \"datetime64[ns]\",\n",
    "    \"tw1_l\": \"datetime64[ns]\",\n",
    "    \"tw2_f\": \"datetime64[ns]\",\n",
    "    \"tw2_l\": \"datetime64[ns]\",\n",
    "    \"en1_f\": \"datetime64[ns]\",\n",
    "    \"en1_l\": \"datetime64[ns]\",\n",
    "    \"en2_f\": \"datetime64[ns]\",\n",
    "    \"en2_l\": \"datetime64[ns]\",\n",
    "    \"en3_f\": \"datetime64[ns]\",\n",
    "    \"en3_l\": \"datetime64[ns]\",\n",
    "    \"la1_f\": \"datetime64[ns]\",\n",
    "    \"la1_l\": \"datetime64[ns]\",\n",
    "    \"la2_f\": \"datetime64[ns]\",\n",
    "    \"la2_l\": \"datetime64[ns]\",\n",
    "    \"la3_f\": \"datetime64[ns]\",\n",
    "    \"la3_l\": \"datetime64[ns]\",\n",
    "    \"la4_f\": \"datetime64[ns]\",\n",
    "    \"la4_l\": \"datetime64[ns]\",\n",
    "    \"ps1_f\": \"datetime64[ns]\",\n",
    "    \"ps1_l\": \"datetime64[ns]\",\n",
    "    \"rel_weir_time_s\": int,\n",
    "    \"pool_stage\": str,\n",
    "    \"censor\": int,\n",
    "    \"altered\": int,\n",
    "    \"mort_xlat\": int,\n",
    "    \"release_stage\": str,\n",
    "    \"comments\": str\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in beacon files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in beacon file\n",
    "beacon_df = pd.read_csv(beacon_file)\n",
    "beacon_df.rename(columns={i:str(i).lower() for i in beacon_df.columns}, inplace=True)\n",
    "beacon_df.rename(columns={'hits_seconds_run1': 'hits_seconds_run'}, inplace=True)\n",
    "\n",
    "# # build a dictionary of receiver id to detect site\n",
    "reciever_to_detect_site_dict = beacon_df.set_index(\"receiver_id\")[\"detect_site\"].to_dict()\n",
    "\n",
    "# construct a dictionary of receiver id to site number\n",
    "receiver_to_site_number_dict = beacon_df.set_index(\"receiver_id\")[\"site_number\"].to_dict()\n",
    "\n",
    "# build a dict of site number to receiver id\n",
    "site_to_receiver_dict = beacon_df.set_index(\"site_number\")[\"receiver_id\"].to_dict()\n",
    "\n",
    "# read in tagging data\n",
    "tagging_df = read_tagging_file(tagging_file)\n",
    "\n",
    "# create a list of valid fish ids to process\n",
    "fish_array = tagging_df[\"fish_id\"].unique()\n",
    "\n",
    "# get a dict of PIT tags per fish_id\n",
    "pit_tag_dict = tagging_df.set_index(\"fish_id\")[\"PIT.Tag\"].to_dict()\n",
    "\n",
    "# get a dict of SRR per fish_id\n",
    "srr_dict = tagging_df.set_index(\"fish_id\")[\"srr\"].to_dict()\n",
    "release_name_dict = tagging_df.set_index(\"fish_id\")[\"release_location_xlat\"].to_dict()\n",
    "\n",
    "# there is no lot in the tagging file; using srr as a proxy\n",
    "lot_dict = tagging_df.set_index(\"fish_id\")[\"srr\"].to_dict()\n",
    "\n",
    "activation_dict = tagging_df.set_index(\"fish_id\")[\"tag_activation_date_pst\"].to_dict()\n",
    "release_dict = tagging_df.set_index(\"fish_id\")[\"tag_release_date_pst\"].to_dict()\n",
    "mort_dict = tagging_df.set_index(\"fish_id\")[\"mort_xlat\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N unique frequencies =  5\n",
      "N unique codes =  [100 101 102 108 109 110 116 117 118 124 125 132 133 140 141 148 149]\n"
     ]
    }
   ],
   "source": [
    "# generate lists of expected frequencies and codes from tagging file\n",
    "fish_id_array = tagging_df[\"fish_id\"].values\n",
    "target_frequency_list = np.unique([float(i[:7]) for i in fish_id_array])\n",
    "target_code_list = np.unique([int(i[-3:]) for i in fish_id_array])\n",
    "\n",
    "print('N unique frequencies = ', len(target_frequency_list))\n",
    "print('N unique codes = ', target_code_list)\n",
    "\n",
    "# generate a list of expected site numbers from the beacons file\n",
    "target_site_number_list = beacon_df[\"site_number\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert native input files to parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the MITAS and ORION files have been built, they do not need to be built again unless new data is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:04<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate formatted raw files into parquet format for query\n",
    "mitas_raw_parquet_files = generate_mitas_parquet_files(mitas_dir=mitas_dir,\n",
    "                                                       target_frequency_list=target_frequency_list,\n",
    "                                                       target_code_list=target_code_list, \n",
    "                                                       output_directory=parquet_raw_dir,\n",
    "                                                       output_tz='US/Pacific',\n",
    "                                                       target_year=target_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//pnl/projects/Toutle/Data/field_data/01_fall_2024/mitas/20241031_20241102.csv\n",
      "20241031_20241102\n",
      "2024-11-04 00:00:00\n",
      "2024-11-04 00:00:00-08:00\n"
     ]
    }
   ],
   "source": [
    "input_file='//pnl/projects/Toutle/Data/field_data/01_fall_2024/mitas/20241031_20241102.csv'\n",
    "print(input_file)\n",
    "fnm = os.path.splitext(os.path.basename(input_file))[0]\n",
    "print(fnm)\n",
    "fnm_date = fnm.split('_')[-1]\n",
    "download_date = datetime.datetime.strptime(fnm_date, '%Y%m%d') + datetime.timedelta(days=1)\n",
    "print(download_date)\n",
    "download_date = pytz.timezone('US/Pacific').localize(download_date)\n",
    "print(download_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 21/21 [00:11<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate formatted raw files into parquet format for query\n",
    "orion_raw_parquet_files = generate_orion_parquet_files(orion_dir=orion_dir,\n",
    "                                                       target_frequency_list=target_frequency_list,\n",
    "                                                       target_code_list=target_code_list, \n",
    "                                                       output_directory=parquet_raw_dir,\n",
    "                                                       site_to_receiver_dict=site_to_receiver_dict,\n",
    "                                                       target_year=target_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_ORION_S11_20241211.parquet is empty\n",
      "Empty DataFrame\n",
      "Columns: [fish_id, date_time, receiver_id, signal_power, file_nm]\n",
      "Index: []\n",
      "0_ORION_S3_20241211.parquet is empty\n",
      "Empty DataFrame\n",
      "Columns: [fish_id, date_time, receiver_id, signal_power, file_nm]\n",
      "Index: []\n",
      "0_ORION_S5_20241211.parquet is empty\n",
      "Empty DataFrame\n",
      "Columns: [fish_id, date_time, receiver_id, signal_power, file_nm]\n",
      "Index: []\n",
      "0_ORION_S6_20241211.parquet is empty\n",
      "Empty DataFrame\n",
      "Columns: [fish_id, date_time, receiver_id, signal_power, file_nm]\n",
      "Index: []\n",
      "0_ORION_S7_20241211.parquet is empty\n",
      "Empty DataFrame\n",
      "Columns: [fish_id, date_time, receiver_id, signal_power, file_nm]\n",
      "Index: []\n",
      "0_ORION_S8_20241211.parquet is empty\n",
      "Empty DataFrame\n",
      "Columns: [fish_id, date_time, receiver_id, signal_power, file_nm]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "for f in orion_raw_parquet_files:\n",
    "    fnm = os.path.basename(f)\n",
    "    odf = duckdb.read_parquet(f).df()\n",
    "    if len(odf) == 0:\n",
    "        print(f\"{fnm} is empty\")\n",
    "        print(odf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating percentile:  0.0\n",
      "Signal power thresholds:\n",
      "    site_number  signal_power  percentile\n",
      "7             1          -105           0\n",
      "10            2          -104           0\n",
      "5             3          -104           0\n",
      "4             4          -104           0\n",
      "3             5          -104           0\n",
      "0             6          -104           0\n",
      "2             7          -104           0\n",
      "6             8          -104           0\n",
      "1             9          -104           0\n",
      "8            10          -104           0\n",
      "9            11          -104           0\n",
      "Writing travel time file...\n",
      "CPU times: total: 219 ms\n",
      "Wall time: 1min 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "<timed exec>:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# target percentiles to evaluate for signal power threshold; 0.0 threshold is equivalent to no threshold\n",
    "#percentile_list = [0.8]\n",
    "percentile_list = [0.0]\n",
    "\n",
    "for percentile in percentile_list:\n",
    "    \n",
    "    print(f\"Simulating percentile:  {percentile}\")\n",
    "\n",
    "    # all processed files\n",
    "    glob_path = os.path.join(parquet_raw_dir, \"*.parquet\")\n",
    "\n",
    "    # use all detections to generate a target percentile of signal power for each receiver\n",
    "    sig_df = duckdb.query(f\"\"\"SELECT receiver_id, quantile(signal_power, {percentile}) as power FROM '{glob_path}' GROUP BY receiver_id\"\"\").df()\n",
    "\n",
    "    # add detect site\n",
    "    sig_df[\"detect_site\"] = sig_df[\"receiver_id\"].map(reciever_to_detect_site_dict)\n",
    "\n",
    "    # change receiver id to site number\n",
    "    sig_df[\"site_number\"] = sig_df[\"receiver_id\"].map(receiver_to_site_number_dict)\n",
    "\n",
    "    # keep only valid receivers\n",
    "    sig_df = sig_df.loc[~sig_df[\"site_number\"].isna()]\n",
    "\n",
    "    # convert site ids to integers to facilitate lookup\n",
    "    sig_df[\"detect_site\"] = sig_df[\"detect_site\"].astype(int)\n",
    "    sig_df[\"site_number\"] = sig_df[\"site_number\"].astype(int)\n",
    "\n",
    "    # generate a dictionary of site id to its power threshold dictionary\n",
    "    site_to_power_percentile_dict = sig_df.set_index(\"site_number\")[\"power\"].to_dict()\n",
    "\n",
    "    # generate a data frame to display the signal power threshold per receiver\n",
    "    xd = {\"site_number\": [], \"signal_power\": [], \"percentile\": []}\n",
    "    for x in site_to_power_percentile_dict.keys():\n",
    "\n",
    "        xd[\"site_number\"].append(x)\n",
    "        xd[\"signal_power\"].append(site_to_power_percentile_dict[x])\n",
    "        xd[\"percentile\"].append(int(percentile * 100))\n",
    "\n",
    "    xf = pd.DataFrame(xd).sort_values(by=[\"site_number\"])\n",
    "    print(\"Signal power thresholds:\")\n",
    "    print(xf)\n",
    "    \n",
    "    # construct simulation name    \n",
    "    simulation = f\"toutle_fall_2024_{int(percentile * 100)}-percentile-power\"\n",
    "    #simulation = f\"mm_sum_2024_411-percentile-power\"\n",
    "\n",
    "    # generate fish list to process\n",
    "    fish_list = tagging_df[\"fish_id\"].unique()\n",
    "    \n",
    "    results = Parallel(n_jobs=-2, backend=\"loky\")(delayed(process_parallel)(fish_id=i,\n",
    "                                                                            output_directory=output_dir,\n",
    "                                                                              parquet_raw_dir=parquet_raw_dir,\n",
    "                                                                              travel_time_template_dtypes=travel_time_template_dtypes,\n",
    "                                                                              reciever_to_detect_site_dict=reciever_to_detect_site_dict,\n",
    "                                                                              receiver_to_site_number_dict=receiver_to_site_number_dict,\n",
    "                                                                              signal_power_threshold_dict=site_to_power_percentile_dict,\n",
    "                                                                              detect_site_to_abbrev_dict=detect_site_to_abbrev_dict,\n",
    "                                                                              detection_site_abbrev_list=detection_site_abbrev_list,\n",
    "                                                                              detect_plot_name_dict=detect_plot_name_dict,\n",
    "                                                                              site_plot_name_dict=site_plot_name_dict,\n",
    "                                                                              simulation=simulation,\n",
    "                                                                              tagging_df=tagging_df,\n",
    "                                                                              beacon_df=beacon_df,\n",
    "                                                                              project_end_date=project_end_date,\n",
    "                                                                              max_tag_life_days=max_tag_life_days,\n",
    "                                                                              use_events=True, \n",
    "                                                                              consider_dam_operations=False,\n",
    "                                                                              output_plot_directory=os.path.join(output_dir, f\"{simulation}/events/events_plots\"),\n",
    "                                                                              generate_event_plots=True,\n",
    "                                                                              write_clean_data=True,\n",
    "                                                                              generate_clean_plots=True) for i in fish_list)        \n",
    "\n",
    "    for index, ttdf in enumerate(results):\n",
    "        # add fish to travel time output\n",
    "        if index == 0:\n",
    "            travel_time_df = ttdf\n",
    "        else:\n",
    "            travel_time_df = pd.concat([travel_time_df, ttdf], axis=0)\n",
    "\n",
    "    # save travel time file\n",
    "    print(\"Writing travel time file...\")\n",
    "    travel_time_df.to_csv(os.path.join(output_dir, f\"{simulation}/travel_time/travel_time.csv\"), index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
